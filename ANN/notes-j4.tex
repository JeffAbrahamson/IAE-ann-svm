\input ../notes-header.tex
\usepackage{epsfig}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Notes from last week}

\notesection{OvO, OvA}

OvO : vote

\begin{quote}
  More to compute than OvA.  Still a heuristic.
\end{quote}

OvA : choose classifier with highest score

\begin{quote}
  This heuristic presents several problems.  The classifiers are not
  necessarily comparable.  Also, the binary classifiers see unbalanced
  class ratios (more negatives than positives).
\end{quote}

\notesection{Minibatch vs Stochastic descent}

\begin{itemize}
\item \textbf{SGD.} Update for each sample in the training set
  (repeatedly).
  \begin{itemize}
  \item Often called online
  \item Frequent updates mean quick insight on progress
  \item Sometimes faster learning
  \item Noisy, which can help avoid local minima
  \item \textbf{But\dots} lots of updates
  \item Noisy (cf above)
  \item Sometimes slower
  \end{itemize}
\item \textbf{Batch Gradient Descent.} Train on all samples (one
  epoch), then update.
  \begin{itemize}
  \item Fewer updates, sometimes more efficient
  \item Not so noisy
  \item More easily parallelisable
  \item \textbf{But\dots} stability sometimes means premature
    convergence
  \item Updates are more costly (but have to do them anyway)
  \item Sometimes slow
  \end{itemize}
\item \textbf{Mini-Batch Gradient Descent.} Like batch, but smaller
  epochs (train on a subset each time)
  \begin{itemize}
  \item Goldilocks version
  \item Probably the most common case
  \end{itemize}
\item \textbf{Coordinate Gradient Descent.} Choose one coordinate at a
  time.  Less common.
\end{itemize}

\notesection{Continuous vs discrete}

Make a continuous problem discrete with a threshold.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Rules of Machine Learning}

(source: Martin Zinkevich, Google)

\notesection{Some terms}

\begin{itemize}
\item \textbf{Instance}: The thing about which you want to make a
  prediction. For example, the instance might be a web page that you
  want to classify as either "about cats" or "not about cats".
\item \textbf{Label}: An answer for a prediction task ­­ either the
  answer produced by a machine learning system, or the right answer
  supplied in training data. For example, the label for a web page
  might be "about cats".
\item \textbf{Feature}: A property of an instance used in a prediction
  task. For example, a web page might have a feature "contains the
  word 'cat'".
\item \textbf{Feature Column:} A set of related features, such as the
  set of all possible countries in which users might live. An example
  may have one or more features present in a feature column. "Feature
  column" is Google-specific terminology. A feature column is referred
  to as a "namespace" in the VW system (at Yahoo/Microsoft), or a
  field.
\item \textbf{Example:} An instance (with its features) and a label.
\item \textbf{Model:} A statistical representation of a prediction
  task. You train a model on examples then use the model to make
  predictions.
\item \textbf{Metric:} A number that you care about. May or may not be
  directly optimized.
\item \textbf{Objective:} A metric that your algorithm is trying to optimize.
\item \textbf{Pipeline:} The infrastructure surrounding a machine
  learning algorithm. Includes gathering the data from the front end,
  putting it into training data files, training one or more models,
  and exporting the models to production.
\item \textbf{Click-through Rate:} The percentage of visitors to a web
    page who click a link in an ad.
\end{itemize}


\notesection{Overview}

\begin{itemize}
\item Engineering is more important than ML.  If it's not reliable,
  solid, and reproducible, the rest doesn't matter.
\item Have reasonable objects
\item Be as simple as possible
\end{itemize}


\notesection{Start without ML}

\begin{itemize}
\item ML needs data, you rarely start with lots of data
\item Heuristics will often get you half way there
\item Your first goal is just to be better than random.  So identify
  what random looks like.
\end{itemize}

\notesection{Design and Implement Metrics}

\begin{itemize}
\item Start by measuring, otherwise you can't know how you're doing
\item Measuring the first thing is the hardest
\item People care less early, so less resistance
\item Get historical data now.  When you start to care, you'll have a baseline.
\end{itemize}

\notesection{Prefer ML to complex heuristics}

\begin{itemize}
\item It's more maintainable
\item But have you tried simple heuristics?
\end{itemize}

\notesection{Start with simple models and get infrastructure right}

\begin{itemize}
\item If your pipeline is shoddy, it will be hard to do anything
  anyway
\item The first model provides the biggest delta
\item This is ``hello world'' territory, focus on the basics
  \begin{itemize}
  \item getting data
  \item representing data
  \item identifying good vs bad
  \item how to integrate model into application
  \end{itemize}
\item Simple features are easier to understand, debug
\item Make sure you understand your data
\end{itemize}

\notesection{Test infrastructure separately from ML}

\begin{itemize}
\item Make sure the infra is testable
\item Make sure the ML is encapsulated
\item Test getting data into the system
\item Test that features are populated correctly
\item Inspect the data (if allowed)
\item Compare statistics from your pipeline with other sources (if
  exist)
\item Test moving models from training to production
\item Make sure you understand your data
\end{itemize}

\notesection{Heuristics become features}

\begin{itemize}
\item Often some system already exists.  It uses heuristics, produces
  features.  Take advantage of that.
\item Consider using the existing system as a sort of pre-processor,
  generating synthetic features.
\end{itemize}

\notesection{Monitoring and alerting are important}

\begin{itemize}
\item Understand your freshness requirements
\item Do sanity checks at model export time, at deploy time
\item Understand what requires an email, what requires a page, what
  just has to be available for inspection
\item Be aware of silent failures (e.g., data source decay)
\item Make sure features have owners and that it's documented who they
  are (and that the features are documented).  Same for algorithms.
\end{itemize}

\notesection{Objectives (objective function)}

\begin{itemize}
\item Start simple: at first, many things are correlated
\item Start simple: observable and easily measurable
\item Avoid (at first) indirect effects (next/previous day,
  correlations between features)
\item Don't try to use ML to measure user internal state (happiness,
  satisfaction)
\end{itemize}

\notesection{Interpretable models are easier to debug}

\begin{itemize}
\item Linear, logistic, and poisson regression are directly motivated
  by probabilistic models, so easier to reason about
\item Models with objectives based on 0-1 loss, hinge loss, etc. are
  harder to reason about
\end{itemize}

\notesection{From phase 1 to phase 2}

\begin{itemize}
\item Phase 1 is getting a working end-to-end system
  \begin{itemize}
  \item training data
  \item metrics
  \item infrastructure, pipeline
  \item unit and system tests
  \end{itemize}
\item Phase 2 is feature engineering
  \begin{itemize}
  \item adding and inventing new features
  \item metrics mostly all rising
  \end{itemize}
\end{itemize}

\notesection{Launch (and iterate)}

\begin{itemize}
\item Expect that your first model is not your last: avoid complexity
  that will slow you down later
\item Think about how easy it is to add or remove features
\item Think about how to run multiple copies in parallel
\item Don't sweat the small stuff, you'll do it next iteration (next quarter)
\end{itemize}

\notesection{Start with observed features}

\begin{itemize}
\item That is, don't start with learned features
  \begin{itemize}
  \item features from other systems (different objectives, maybe stale)
  \item features you learn yourself (e.g., clustering)
  \end{itemize}
\item Many algorithms are non-convex, so taking their features might
  kill your convergence (different local minima on different runs)
\item Harder to judge impact of changes
\item So shoot first for good baseline
\end{itemize}

\notesection{Simple feature engineering}

\begin{itemize}
\item Consider fixed intervals (e.g., MLP) rather than variable (e.g.,
  LSTM)
\item Consider discretisation (e.g., age bands), don't worry too much
  about getting the banding right
\item Crosses are useful, but can generate too much data, can
  overfit.  E.g., word in query, word in document
\item You can (roughly) learn as many weights as you have data
\item Clean up features you are no longer using, they are technical debt
\end{itemize}

\notesection{Human analysis of the system}

\begin{itemize}
\item This is more art than science, but it's important!
\item You are not a typical end user
  \begin{itemize}
  \item You are too close to the code and to the problem
  \item Confirmation bias
  \end{itemize}
\item Get (even pay) other people to test, they cost less than
  engineers
\item Think about team bias: are you all white, all male, etc.
\item Watch real people use the system, don't correct them, they're
  right, whatever they do
\end{itemize}

\notesection{Comparing models}

\begin{itemize}
\item Know how to measure delta between models (e.g., what you're
  working on and production)
\item Make sure that comparing a model with itself says small delta
\item Remember that, ultimately, you're optimising a business problem,
  not log loss
\end{itemize}

\end{document}
