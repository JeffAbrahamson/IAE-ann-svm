\input ../notes-header.tex
\usepackage{epsfig}

\begin{document}

Let's look at the problem of recognising hand-written digits. \\
Good toy problem: practical, hard, simple. \\
It will take us the whole lecture to do a first draft on how to solve
this problem.

Humans can do it, so maybe think about how we might do it.


\notetitle{Neural Networks}

ANN
\begin{itemize}
\item Long a curiosity
\item 2012 paper, Hinton, image classification, 1000 categories, 60 million parameters
\item neurones : axone, terminaison de l'axone, noyau, dendrites; activation energy
\end{itemize}

\purple{\fbox{Show a neuron with dendrites, explain how it works.}}

\input{neuron.pdf_t}

\bigskip

What are we modeling?
\begin{enumerate}
\item all or none
\item cumulative influence
\item synaptic weight
\item (not) refractory period \textit{(période réfractaire)}
\item (not) axonal bifurcation
\item (not) time patterns
\end{enumerate}

So we have a model of a neuron (a collection of weights and thresholds).\\
But what about collections of neurons?

\begin{displaymath}
  \mbox{inputs} \rightarrow (w, t) \rightarrow \mbox{outputs}
\end{displaymath}

So we want $z = f(x, w, t)$.  Training means adjusting $w, t$.\\
An ANN is a function approximator.

We want some desired function, $d = g(x)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Types of neurons}

\bigskip
\textbf{Linear neuron}
\begin{displaymath}
  y = b + \sum_i x_i w_i
\end{displaymath}
where
\begin{align*}
  y & = \text{output} \\
  b & = \text{bias} \\
  x_i & = \text{$i^\mathrm{\,th}$ input} \\
  w_i & = \text{weight on $i^\mathrm{\,th}$ input}
\end{align*}


\bigskip
\textbf{Binary threshold neuron}
\begin{align*}
  z & = \sum_i x_i w_i \\[2mm]
  y & = \left\{
      \begin{array}{l}
        1  \text{ if } z \ge 0 \\
        0  \text{ otherwise}        
      \end{array}
  \right.
\end{align*}
where
\begin{align*}
  z & = \text{total input} \\
  y & = \text{output} \\
  x_i & = \text{$i^\mathrm{\,th}$ input} \\
  w_i & = \text{weight on $i^\mathrm{\,th}$ input}
\end{align*}


\bigskip
\textbf{Rectified linear neuron}
\begin{align*}
  z & = b + \sum_i x_i w_i \\[2mm]
  y & = \left\{
      \begin{array}{l}
        z  \text{ if } z \ge 0 \\
        0  \text{ otherwise}        
      \end{array}
  \right.
\end{align*}
where
\begin{align*}
  z & = \text{total input} \\
  y & = \text{output} \\
  b & = \text{bias} \\
  x_i & = \text{$i^\mathrm{\,th}$ input} \\
  w_i & = \text{weight on $i^\mathrm{\,th}$ input}
\end{align*}


\bigskip
\textbf{Sigmoid neuron}
\begin{align*}
  z & = b + \sum_i x_i w_i \\[2mm]
  y & = \frac{1}{1+e^{-z}}
\end{align*}
\textit{(This is differentiable.)}


\bigskip
\textbf{Stochastic binary neuron}
\begin{align*}
  z & = b + \sum_i x_i w_i \\[2mm]
  p &= \frac{1}{1+e^{-z}} \\[3mm]
  y & = \left\{
      \begin{array}{l}
        1  \text{ with probability } p \\[2mm]
        0  \text{ with probability } 1 - p
      \end{array}
  \right.
\end{align*}
\textit{(a probability distribution)}

\blue{\it Can also do something similar with rectified linear
  neurons, produce spikes with probability $p$ with a Poisson
  distribution.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Neural Networks}

\textbf{Architecture} is how we connect the states.

\textbf{Feed forward network}
\begin{itemize}
\item Flow is unidirectional
\item No loops
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Too simple example: perceptron}

Let's take a quick detour into something that looks like a neural
network, is very simple, but exhibits a different sort of complexity
than our simple neuron.  (This is mostly of historical interest, to
understand the evolution of the field.)

Perceptron is a supervised algorithm.  Given inputs
$D = \{(x_j, d_j)\}$, want classifier $f$.

We'll call $x_{j,i} = x^{(j)}_i$ the value of feature $i$ in training datum $j$.
We set $x_{j,0}\equiv 1$.  This way we don't have to think about bias.

\textbf{Perceptron algorithm:}
\begin{displaymath}
  \mbox{At each output, }
  f(x; w, b) = \left\{
    \begin{array}{ll}
      1 & \mbox{if } w\cdot x + b > 0 \\
      0 & \mbox{otherwise}
    \end{array}
 \right.
\end{displaymath}
\begin{itemize}
\item Initialise weights randomly.
\item $\forall x_j\in \mbox{inputs}$, compute output: $y_j(t) = f(w(t)\cdot x_j)$
\item Update weights
\end{itemize}

A bit more detail on weight update:

\fbox{v1}
\begin{displaymath}
w(t+1) = \left\{
\begin{array}{ll}
  w(t) - r(t) & \mbox{ if false positive} \\
  w(t) & \mbox{ if no error} \\
  w(t) + r(t) & \mbox{ if fail to recognise} \\
\end{array} \right.
\end{displaymath}

\fbox{v2} ($d=$ desired, $y=$ observed) :
\begin{displaymath}
  d(t) - y(t) = \left\{
\begin{array}{ll}
  -1 & \mbox{ if  false positive} \\
  0 & \mbox{ if no error} \\
  1 & \mbox{ if fail to recognise} \\
\end{array} \right.
\end{displaymath}

\fbox{v3} $w_i(t+1) = w_i(t) + r(t)\cdot (d_j - y_j(t)) x_{j,i}$ for all features $i$

The weight are updated at the last step after each training sample.

Converges if separable.\\
Note we don't need a learning rate.

A perceptron with two inputs looks like this:
\begin{displaymath}
  w_1 x_1 + w_2 x_2 = T
\end{displaymath}
(where $T$ is a threshold).  So this is a line.


\bigskip
\textbf{Note:}

It turns out that the perceptron algorithm is unstable and prone to
many problems on deep or non-linear networks.


\bigskip
\textbf{Note:}

We also call the separating plane (hyperplane) a decision boundary. \\
The function we also call a discriminant or decision function.\\
At the boundary, the decision function is zero.

Recall geometry: decision boundary is perpendicular to weight vector.

\begin{proof}
  Consider $x_1$ and $x_2$ on the decision boundary.  Then
  $f(x_1; w, b) = 0$ and $f(x_2; w, b) = 0$.  So
  $w\cdot (x_1 - x_2) = 0$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Next Steps}

Idea: maybe add some layers in the middle.

What would we put there?

Maybe choose not to care, call them ``hidden layers''.

Neuron activity at each layer must be a non-linear function of
previous layer

\red{If more than two hidden layers, then we call it deep}

\blue{How do we train such a thing?  We'll see that in more detail next week.}

\bigskip
\textbf{Activation function:} How the neuron decides when to fire.

\bigskip
\textbf{Things we do in the middle.}
\begin{itemize}
\item sigmoid neurons
\item ReLU neurons
\item ReLU + softmax neurons
\item Pooling, e.g. max (a form of down sampling)
\item Dropout
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Perceptron and MNIST}

MNIST is the ``hello world'' of ML.

Talk about OvO, OvA.  Introduce voting.

Note that many problems admit multiple solutions.  The perceptron will
pick one, but not necessarily the best.  The ``perceptron of optimum
stability'' (SVM) was designed to solve this problem.

ANNs don't necessarily find the best solution.

\textbf{Multilayer Perceptron (MLP)}
\begin{itemize}
\item Not perceptron
\item Multiple layers
\item First layer is linear
\item Later layers use non-linear activation functions (typically sigmoid)
\item Feedforward
\item Can find non-linear separators
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\notetitle{Training a single neuron}

\medskip
\input{neuron.pdf_t}
\medskip

Let's start with a performance function $P=\| d-z \|$.  \\
Why don't we like that?
% Because it's not smooth, the mathematics isn't nice.

So instead use $P=\| d-z \|^2 = (d-z)^2$.

\textit{(Draw $w_1$-$w_2$ plot with isoclines.  Show four candidate
  steps.  Mention 60 million parameters in Hinton's model.  Talk about
  exponential explosion with number of weights.)}

So we can follow the gradient:
\begin{displaymath}
  \frac{\partial P}{w_1} \quad , \quad   \frac{\partial P}{w_2}
\end{displaymath}

\begin{displaymath}
  \Delta w = r\left( \frac{\partial P}{w_1} \mathbf{i} +
    \frac{\partial P}{w_2} \mathbf{j} \right)
\end{displaymath}

What goes wrong?\\

\textit{Computer scientists were stuck on this for a quarter century.
Then Paul Werbos showed in his 1974 PhD dissertation how to train
neural networks using back-propagation of errors.}

\textbf{Two problems:}

\textbf{\large 1. } Thresholds are annoying.  We don't want $z = f(x, w, T)$
but $z=f(x, w)$.

So let's add an extra weight $w_0$ and input $x_0=-1$.  Then set $w_0\equiv T$.

\textit{(Show that this moves the threshold step to 0.  So we don't
  have to pay attention to it anymore.)}

\textbf{\large 2. } We're using a step function, which isn't continuous.

Let's smooth it out.  Use sigmoid function.
\begin{displaymath}
  \beta = \frac{1}{1+e^{-\alpha}}
\end{displaymath}

\textit{(Walk through how to graph this.)}

\textbf{Summary: } What have we done ?
\begin{itemize}
\item Built a neuron
\item Described how to do gradient descent
\item Modified our activation function to be compatible with gradient descent
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Training two neurons}

\begin{itemize}
\item 1 neuron isn't a net
\item 2 neurons is a  net
\end{itemize}

\textbf{This is a simplest possible neural network.}

\input{neuron2.pdf_t}

(Note that $P_1$ and $P_2$ are products.)

We talk about performance, loss, cost function:
\begin{displaymath}
  L = \frac{1}{2} (d-z)^2
\end{displaymath}

So we want to do gradient descent, this means computing partial derivatives.

\begin{displaymath}
  \frac{\partial L}{\partial w_2}
  =
  \frac{\partial L}{\partial z}
  \frac{\partial z}{\partial w_2}
  =
  \frac{\partial L}{\partial z}
  \frac{\partial z}{\partial P_2}
  \frac{\partial P_2}{\partial w_2}
\end{displaymath}

\begin{displaymath}
  \frac{\partial L}{\partial w_1}
  =
  \frac{\partial L}{\partial z}
  \frac{\partial z}{\partial P_2}
  \frac{\partial P_2}{\partial y}
  \frac{\partial y}{\partial P_1}
  \frac{\partial P_1}{\partial w_1}
\end{displaymath}

How do we compute these things?

\begin{displaymath}
  \frac{\partial P_2}{\partial w_2} = y
\end{displaymath}

\begin{displaymath}
  \frac{\partial z}{\partial P_2} = z(1-z) \quad \mbox{(come back to this)}
\end{displaymath}

\begin{displaymath}
  \frac{\partial L}{\partial z} = d-z
\end{displaymath}

How do we compute the derivative of the sigmoid function?


\begin{align*}
  \frac{\partial\beta}{\partial\alpha}
  %
  & = \frac{\partial}{\partial\alpha} \left( \frac{1}{1+e^{-\alpha}} \right) \\[3mm]
  %
  & = \frac{\partial}{\partial\alpha} \left(1+e^{-\alpha}\right)^{-1} \\[3mm]
  %
  & = -\left(1+e^{-\alpha}\right)^{-2} e^{-\alpha} (-1) \\[3mm]
  %
  & = \frac{e^{-\alpha}}{1+e^{-\alpha}} \cdot \frac{1}{1+e^{-\alpha}} \\[3mm]
  %
  & = \frac{\mathbf{1+} e^{-\alpha} \mathbf{-1}}{1+e^{-\alpha}} \cdot \frac{1}{1+e^{-\alpha}} \\[3mm]
  %
  & = \left(\frac{1+e^{-\alpha}}{1+e^{-\alpha}} - \frac{1}{1+e^{-\alpha}}\right)
    \cdot \frac{1}{1+e^{-\alpha}} \\[3mm]
  %
  & = (1-\beta)\beta
\end{align*}

This is pretty cool: the derivative of the sigmoid function is independent of the input!

Note : at each phase, we only need to compute the last three factors.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Towards real life\dots}

In real life, we have more than one of these going on at once.  Sort of like this:

\input{neuron2x2.pdf_t}

And then there are cross-overs with more weights, more multipliers.
So we are again at risk of exponential blow-up: there are an
exponential number of paths through the network.

But note that the dependencies are only by column, so lots of things
are already computed.

In other words,
\begin{itemize}
\item Linear in depth
\item Quadratic in width
\end{itemize}

\exercise{Compute all of the partial derivatives in the $2\times 2$
  network above, assuming the interconnections we've shown.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{MLP}

This is our first step from 2 neurons to millions.

This is a real neural network.  It's not a perceptron.  Geoffrey Hinton apologised.

Back to computing hand-written digits.

$8\times 8$, then $16\times 16$.  Maybe $32\times 32$, but let's do
$28\times 28$ for fun.  We'll assume gray scale with activation in
$[0,1]$.

We'll use sigmoids to make intermediate activates also in $(0,1)$.

Show hidden layers.  Make 16 in each (somewhat arbitrary).

Then $784\times 16 + 16\times 16 + 16 \times 10 = 12960$ weights.
Plus biases, so let's say 13K.

We might hope intermediate layers capture abstraction: say edges then
loops.  This time around, they don't.

Walk through backpropagation here.  Note that $\nabla C$ tells us
which changes matter the most.

Talk about inverse images of neuron outputs.

Talk about strange ways ANNs get things wrong (i.e., they don't
recognise what we do, they can be confident about garbage).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Uber pedestrian accident}

In March 2018 in Phoenix, an Uber self-driving car struck and killed a
woman crossing the street with her bicycle.

\begin{itemize}
\item Radar detected woman 5.6 seconds before impact.  SUV in far
  right lane, pedestrian crossing from the left.  System classified as
  vehicle but didn't recognise that she was moving.
\item Lidar pinged her several times over next few seconds, but
  classification repeatedly changed, so no continuity of recognition.
  It was a different object each time.
\item At 1.5 seconds before impact, woman was partially in SUV lane,
  so system plotted course to steer around woman.
\item Milliseconds later, identification as a bicycle on collision
  course.  Abandons plan, which hadn't taken into account that
  bicycles move faster than pedestrians.
\item Uber had disabled the emergency steering and braking systems
  because erratic.  So SUV began gradual deceleration.  At 1.2 seconds
  before impact, SUV was moving at 65 kph (40 mph).
\item One second later (0.2 seconds before impact), system alerted
  human safety driver that it had initated a controlled slowdown.
  Safety driver intervened, but 0.2 seconds is inadequate for a
  human.  The SUV struck the woman, and then the safety driver engaged
  the brakes.
\item She was not in a crosswalk, and the system had learned to
  identify pedestrians in crosswalks (a correlated feature).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Deep networks}

We want to go from 2 parameters to millions.

\begin{itemize}
\item convolution
\item pooling --- max pooling
\item kernel \textit{(taking some neurons and running them across the image)}
\item multiple kernels
\item softmax
\item dropout
\end{itemize}

We don't really know why any of this works.

Talk about auto-encoders.

\end{document}
