\input ../notes-header.tex
\usepackage{epsfig}
\usepackage{bbm}

\begin{document}

\notetitle{Support Vector Machines}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Thinking in Statistics}

A beer company did live demonstrations during football halftime:
invite a few hundred people who prefer the competitor's beer to a
blind taste test.  Show that half of them actually prefer our beer.

This was done repeatedly on live television.

Discuss.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{How to Have Confidence}

Talk about leave-one-out cross validation.

\begin{itemize}
\item \texttt{train\_test\_split}
\item learning curves
\item cross validation \textit{(did you play with matplotlib: scatter,
  plot?)}
\item LOOCV
\end{itemize}

Careful in all this about what you can compare!

\begin{verbatim}
from sklearn.model_selection import train_test_split
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Review: Logistic Regression}

\begin{defn}[Kolmogorov]
  \begin{displaymath}
    \pr(A\mid B) = \frac{\pr(A\cap B)}{\pr(B)}
  \end{displaymath}
\end{defn}

\begin{example}[validating points]
  Given a set of points and a new point, what is the probability that
  the new point is part of that distribution?
\end{example}

\begin{soln}
  This is actually logistic regression.

  \begin{theorem}{Bayes}
    \begin{displaymath}
      \pr(H\mid D) = \frac{\pr(D\mid H)\, \pr(H)}{\pr(D)}
    \end{displaymath}
  \end{theorem}
  \begin{proof}
    From definition and symmetry.
  \end{proof}

  Changes to odds formulation,
  \begin{displaymath}
    C(H\mid D) =
    = \frac{\pr(D\mid H)}{\pr(D\mid \overline{H})} \, C(H)
  \end{displaymath}

  To make this look linear, we took logs:
  \begin{displaymath}
    \ln(C(H\mid D)) =
    \ln\left( \frac{\pr(D\mid H)}{\pr(D\mid \overline{H})} \right)
    + \ln(C(H))
  \end{displaymath}
  and then we noted that the log prior is basically a constant and we
  can assume the first term, the log likelihood, is roughly a linear
  function of the data.
  \begin{displaymath}
    \ln(C(H\mid D)) = \beta D + \beta_0
  \end{displaymath}
  And then, if we want to do a bit more arithmetic, we can work
  backwards and find our familiar equation for logistic regression.

\end{soln}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\notetitle{Decision boundaries}

Consider these points.  What would be the decision boundaries for
\begin{itemize}
\item logistic regression
\item CART
\item ANN
\end{itemize}

\centerline{\input{decision.pdf_t}}

\bigskip

Consider a sort of ``ideal'' decision boundary.  Stay linear, but
which line?

Concept of widest street (or: bicycle lane).

How do we make a decision boundary that will use that line?

\bigskip
\centerline{\input{decision-svm1.pdf_t}}
\bigskip

Consider a vector $w$ that is $\bot$ to the separator that we want.
(We don't know its length.)

Let $u$ be a vector pointing to some unknown point that we want to
classify.

Is $u$ on the $+$ or the $-$ side of the decision boundary (zone, margin)?

Let's project $u$ onto $w$.  Then we want
\begin{displaymath}
  w\cdot u \ge c
\end{displaymath}

which is the same as saying this, which we'll call our decision rule:
\begin{equation}
  \label{eq:decision-rule}
  \boxed{w\cdot u + b \ge 0 \Rightarrow +}
\end{equation}

Said differently, the equation of the hyperplane separator is
$w\cdot u + b = 0$.

But we don't know $b$ or $w$.

We want to add contraints so that we can calculate them.

Pick positive sample point $x_+$, negative sample point $x_-$.

Want
\begin{align*}
  w\cdot x_+ + b & \ge 1
  \\
  w\cdot x_- + b & \le -1
\end{align*}
Note that the $\pm 1$ indicates the margin.  Zero is the decision
boundary (the middle line).

For convenience, introduce
\begin{equation*}
  y_i =
  \begin{cases}
    1 & \mbox{positive samples} \\
    -1 & \mbox{negative samples}
  \end{cases}
\end{equation*}

Multiplying by $y_i$, we get
\begin{align*}
  y_i(w\cdot x_+ + b) & \ge 1
  \\
  y_i(w\cdot x_- + b) & \ge 1
\end{align*}

These are the same!
\begin{displaymath}
  y_i(w\cdot x_i + b) - 1 \ge 0
\end{displaymath}

So for $x_i$ on the margin, we have
\begin{equation}
  \label{eq:on-margin}
  \boxed{y_i(w\cdot x_i + b) - 1 = 0}
\end{equation}

We want to maximise the margin.  But what is its width?

If we have a unit vector, we could set the width.  But we can subtract $x_+-x_-$:
\begin{equation}
  \label{eq:width-raw}
  \mbox{width} = (x_+ - x_-) \cdot \frac{w}{\parallel w\parallel}
\end{equation}

But we have
\begin{displaymath}
  1(w\cdot x_+ + b) - 1 = 0 \iff w\cdot x_+ = 1-b
\end{displaymath}
and
\begin{displaymath}
  -1(w\cdot x_- + b) - 1 = 0 \iff -w\cdot x_- = 1+b
\end{displaymath}

So combining with eq. (\ref{eq:width-raw}), we get
\begin{equation}
  \label{eq:width}
  \mbox{width} = \frac{2}{\parallel w\parallel}
\end{equation}

We want to maximise (\ref{eq:width}).

So we can maximise $\frac{1}{\parallel w\parallel}$ $\Rightarrow$
minimise $\parallel w\parallel$ $\Rightarrow$
minimize $\;\frac 12 \parallel w\parallel^2$.

\bigskip

Let's use the Lagrangian!

\red{\fbox{Talk about this, but skip the detail unless asked.}}
\bigskip

\begin{equation}
  \label{eq:lagrangian}
  L = \frac 12 \parallel w\parallel^2 - \sum \lambda_i \left[ y_i(w\cdot
    x_i + b) - 1 \right]
\end{equation}

(It will turn out that $\lambda$ is non-zero only on the margin.)

\begin{displaymath}
  \frac{\partial L}{\partial w} = w - \sum\lambda y_i x_i = 0
\end{displaymath}
and so \hspace{2mm} \red{\fbox{(show $w$ and note dependence on samples)}}
\begin{equation}
  \label{eq:w}
  w = \sum\lambda_i y_i x_i
\end{equation}

So $w$ is a linear combination of the samples!

In addition,
\begin{displaymath}
  \frac{\partial L}{\partial b} = -\sum \lambda_i y_i = 0
\end{displaymath}
and so
\begin{equation}
  \label{eq:b}
  \sum \lambda_i y_i = 0
\end{equation}

So what happens to $L$ (eq. \ref{eq:lagrangian})?

\bigskip

\begin{equation}
  \label{eq:learning-rule}
  L = \frac 12 \left( \sum \lambda_i y_i x_i\right)
  \left(\sum \lambda_j y_j x_j\right)
  -
  \sum \lambda_i y_i x_i
  \left(\sum \lambda_j y_j x_j\right)
  - \boxed{\sum \lambda_i y_i} b + \sum \lambda_i
\end{equation}
The boxed portion is zero, so this leaves us with
\hspace{2mm} \red{\fbox{pick up again here.}}

\begin{equation}
  \label{eq:recognition-rule}
  \sum\lambda_i - \frac 12 \sum_i \sum_j \lambda_i \lambda_j y_i y_j
  x_i\cdot x_j
\end{equation}

This is the optimisation: the part where we learn.

So the optimisation depends on on the dot products of pairs of
samples!

\bigskip

Let's go back to our decision rule, eq. (\ref{eq:decision-rule}).

\begin{displaymath}
  \sum \lambda_i y_i x_i\cdot u + b \ge 0 \Rightarrow +
\end{displaymath}
Thi is the the decision boundary: it only depends on the sample vector
and unknown!

It can be shown that this is a convex space, so we can't get stuck at
local maxima.

We've seen that we can learn and decide using only dot products among
sample data points and between sample data points and unknown (to be
classified) points.  This is the property that lets us use kernels.

\bigskip

Next week we'll come back to this and talk about soft margins and
about kernels.

\bigskip


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\notetitle{Scikit Learn Notes}

\begin{verbatim}
model.fit
model.predict
\end{verbatim}

There are toy data sets to play with.

\begin{verbatim}
import sklearn.datasets as skds
skds.fetch_covtype
\end{verbatim}


\end{document}
